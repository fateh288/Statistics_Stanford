Segmenting the predictor space into a number of simple regions.
Also called decision tree methods
Useful for intepretation, may not give that good prediction accuracy as normal supervised learning approaches.
Tree Building process-
1.Divide predictor space i.e. set of values of X1,X2...Xp into J distinct and non overlapping regions, R1,R2...Rj
2.For every observation that falls into region Rj, we make the same prediction i.e mean of response values for training observations in Rj.

Regions could have any shape.Rectangles chosen for ease of interpretaion and simplicity.
Goal is to find boxes R1,...Rj that minimize RSS given by sigma(sigma(y-y^))
However the above is computationally infeasible.Therefore we take top down greedy approach known as recursive binary splitting.
Select Xj and cutpoint s that leads to greatest reduction in RSS.Repeat process within each of these regions to decrease RSS.The process continues until a stopping critereon is reached e.g.no regions contains more than five observations.
Test value is calculated by taking mean of training in that region.
Pruning a tree:
Why? because overfitting
Smaller tree with fewer splits might lead to lower variance and better interpretation at cost of little bias.
Have a min threshhold on decrease in RSS to stop groeing the tree.

Better strategy-
grow very large tree, and then prune.Cost complexity pruning.Also called weakest link pruning.

A higher value of alpha corresponds to a higher penalty on the complexity of the tree. This means that T1 will have at least as many nodes as T2.

Tree Algorithm-
1.Use recursive biary splitting to grow a large tree on the training data, stopping when each terminal node has fewer than some minimum number of observations.
2.Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of alpha.
3.Use k-fold validation to choose alpha for each k=1,2...k:  repeat steps 1 and 2 on the (k-1)/k th fraction of training data excluding the kth fold.Evaluate the mean squared prediction error on the data in the left out kth fold as a function of alpha.Average the results and pick alpha to minimize the average error.
4.Return the subtree from step 2 that corresponds to chosen value of alpha

CLASSIFICATION TREES:
predict based on most commonly occuring class in the region.
RSS not used.
classification error rate-fraction of training not in most common class.Very noisy measure.
Other measures-
Gini index-measure of total variance across k classes.Also called purity measure.

BAGGING:
Bootstrap aggregation or bagging- procedure for reducing the variance of stats learning method.

Averaging a set of observations reduces variance by factor of n.

We generate B different bootstrapped training data sets. We then train our method on bootstrapped training set.Then average all.This is called bagging.

Out-of-Bag Error Estimation-
On average, each bagged tree makes use of around 2/3 of observations.The reamining 1/3 rd not used in given bagged tree are referred as out of bag(OOB) observations.It is essentially LOO cross validation error for bagging , if B is large.

Random Forest:
It decorrelates the trees.This reduces the variance when we average the trees.
Each time a split is considered, random selection of m predictors is chosen as split candidates from set of p predictors.
Fresh selection of m(approx sqrt(p)) predictors at each split. m=p => bagging.
A lot of chances given to each of predictors.
Predictors having largest variance(irrespective of class i.e. unsupervised) are chosen for prediction.
By looking at out of bag errors we decide when to stop.

BOOSTING:-
Simlilarity with random forest: each tree built on bootstrap dataset independent of other trees.
Difference:
The trees are added sequentially in order to improve the performance of existing set of trees.

Boosting for regression-
1.set avg=0; residual= observations yi;
2.for b=1...B
Fit a tree with d splits.
Update current function and the residuals.
3. Output the boosted model.

Boosting learns slowly and prevents overfitting.
Residuals and function changes slowly.

Tuning parameters for boosting-
1.Number of trees B:
Overfit if B very large.Use cross validation to select B.
2.Shrinkage parameter (lambda):
Rate at which boosting learns.Very small lambda can require very large B for good performance.
3.number of splits (d):
complecity of boosted esemble.if d=1---stump, single split, results in additive model. d is also called interaction depth, d splits => max d variables.

Variable importance measure:
A large value decrease in RSS over all splits indicates important predictor.
For classification trees, total amount that Gini index is decreased by splits over a given predictor, averaged over all B trees.
